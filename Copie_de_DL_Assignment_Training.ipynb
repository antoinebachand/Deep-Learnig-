{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de DL_Assignment_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antoinebachand/Deep-Learnig-/blob/main/Copie_de_DL_Assignment_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF1SAOPoV2dr"
      },
      "source": [
        "By: Antoine Bachand & Edouard Virot & Jurriaan Simon Berger\n",
        "\n",
        "> Bloc en retrait\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ42fw-hXP6I"
      },
      "source": [
        "# Some logistics\n",
        "Setting up stuff in Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtD12-_kTKwY",
        "outputId": "392b48fc-e898-45c5-b014-b32acaebe525"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "ROOT = '/content/drive'\n",
        "drive.mount(ROOT)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm583JeFUKqN"
      },
      "source": [
        "from os.path import join\n",
        "PROJ = 'MyDrive/Deep Learning/Assignment'\n",
        "PROJ_PATH = join(ROOT, PROJ)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oZMoG9ZUV-F",
        "outputId": "0d20811e-6369-4e39-9017-35269b81baba"
      },
      "source": [
        "!rsync -aP \"{PROJ_PATH}\"/* ./"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sending incremental file list\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm5dhzanUYFD"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn.functional as functional\n",
        "import torch.nn as nn"
      ],
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK-L9NI4KEH_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dffAiJfkXYXC"
      },
      "source": [
        "# Loading data\n",
        "\n",
        "Note that the data has been edited already a bit. We created a table that has one 'vector' per company per day. To the vector the sector information and expert judgement have been added. We also did a few data translations: \n",
        "\n",
        "*   Creating a one-hot encoding for the company. Instead of using 0/1/2, where the number is only categorical and does not represent a value.\n",
        "*   Calculating the *Y* values, the values that we need to predict. True for an increase, false for constant or decrease.\n",
        "\n",
        "In this section we will do some more data cleaning and standardization. These steps will be added to the predictor.py file that we hand in. To make sure that the same model can be used by that program, with the similar formatting of the inputs.\n",
        "This also holds for the editing that has been done outside of this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK9ExuqGUcLq"
      },
      "source": [
        "#load the data\n",
        "df = pd.read_csv('data.csv')"
      ],
      "execution_count": 340,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "UaRGSOPbUpEv",
        "outputId": "26a07b29-77ed-421d-cca3-95fcd32f8d65"
      },
      "source": [
        "#inspect if everything is OK\n",
        "df.head()\n"
      ],
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>company</th>\n",
              "      <th>year</th>\n",
              "      <th>day</th>\n",
              "      <th>quarter</th>\n",
              "      <th>expert1</th>\n",
              "      <th>expert2</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>m1</th>\n",
              "      <th>m2</th>\n",
              "      <th>m3</th>\n",
              "      <th>m4</th>\n",
              "      <th>company0</th>\n",
              "      <th>company1</th>\n",
              "      <th>company2</th>\n",
              "      <th>segment</th>\n",
              "      <th>trend</th>\n",
              "      <th>stock-price</th>\n",
              "      <th>price increase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2017</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>6.3</td>\n",
              "      <td>1824.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>102.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2017</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>5.1</td>\n",
              "      <td>6912.0</td>\n",
              "      <td>-0.9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>102.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2017</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>6.6</td>\n",
              "      <td>8928.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>102.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>2017</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7.8</td>\n",
              "      <td>6924.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>102.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2017</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>-0.9</td>\n",
              "      <td>5635.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>102.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   company  year  day  quarter  ...  segment  trend  stock-price  price increase\n",
              "0        0  2017    3        0  ...        1      0        102.2               0\n",
              "1        0  2017    4        0  ...        1      0        102.2               0\n",
              "2        0  2017    5        0  ...        1      0        102.2               0\n",
              "3        0  2017    6        0  ...        1      0        102.2               0\n",
              "4        0  2017    9        0  ...        1      0        102.2               0\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 341
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuN6BQzQWvbP"
      },
      "source": [
        "# TODO\n",
        "# Get rid of columns that we do not want to use\n",
        "# Normalization"
      ],
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOaeV__r-BmP"
      },
      "source": [
        "\n",
        "\n",
        "*   Drop the column 'company' > this information is available in the one-hot encoding\n",
        "*   Drop the column with the year, day and quarter. This is 'categorical' data/time series, for which the model cannot account.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LlSXoZB8Zzp"
      },
      "source": [
        "# TODO:\n",
        "# Quarter could maybe work, day very very maybe\n",
        "# Perhaps change it in day of the week (with one-hot encoding)\n",
        "\n",
        "df_drop = df.drop(['company','year','day','quarter'],axis=1)"
      ],
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PZUEQ8uy-_67",
        "outputId": "86913d00-4370-433b-a025-392dd5942dc1"
      },
      "source": [
        "df_drop.head()"
      ],
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>expert1</th>\n",
              "      <th>expert2</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>m1</th>\n",
              "      <th>m2</th>\n",
              "      <th>m3</th>\n",
              "      <th>m4</th>\n",
              "      <th>company0</th>\n",
              "      <th>company1</th>\n",
              "      <th>company2</th>\n",
              "      <th>segment</th>\n",
              "      <th>trend</th>\n",
              "      <th>stock-price</th>\n",
              "      <th>price increase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>6.3</td>\n",
              "      <td>1824.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>102.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>5.1</td>\n",
              "      <td>6912.0</td>\n",
              "      <td>-0.9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>102.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>6.6</td>\n",
              "      <td>8928.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>102.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7.8</td>\n",
              "      <td>6924.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>102.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>-0.9</td>\n",
              "      <td>5635.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>102.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   expert1  expert2  sentiment  ...  trend  stock-price  price increase\n",
              "0        0        0         10  ...      0        102.2               0\n",
              "1        0        1         10  ...      0        102.2               0\n",
              "2        0        1         10  ...      0        102.2               0\n",
              "3        0        1         10  ...      0        102.2               0\n",
              "4        0        1         10  ...      0        102.2               0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 344
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACCSHNlz-Tvt"
      },
      "source": [
        "*    Scale all columns 0 to 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYh7k26x9tnO"
      },
      "source": [
        "# TODO:\n",
        "# Does this scaling work properly?\n",
        "# How to make sure that the same scaling is applied in the final model?\n",
        "\n",
        "# Look into: https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range \n",
        "\n",
        "# Using normalization, as the data does not follow a Gaussian distribution. Also we use a NN. From: https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/ \n",
        "\n",
        "norm = MinMaxScaler().fit(df_drop) # In the example, Xtrain is the input for this; only fit it on the correct columns\n",
        "nd_norm = norm.transform(df_drop)\n",
        "df_norm = pd.DataFrame(nd_norm)\n",
        "df_norm.columns = ['expert1', 'expert2', 'sentiment', 'm1','m2', 'm3', 'm4', 'company0', 'company1', 'company2', 'segment', 'trend', 'stock-price', 'price increase']\n",
        "\n",
        "# Not used now, note that the one-hot encoding columns and price increase should be treated apart.\n",
        "scaler = StandardScaler().fit(df_drop)\n",
        "nd_scaled = scaler.transform(df_drop)\n",
        "df_scaled = pd.DataFrame(nd_scaled)\n"
      ],
      "execution_count": 345,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "C7N9nmCn-8lt",
        "outputId": "d74ef71d-9f79-4230-d206-7daf8b0791f8"
      },
      "source": [
        "df_norm.describe()"
      ],
      "execution_count": 346,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>expert1</th>\n",
              "      <th>expert2</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>m1</th>\n",
              "      <th>m2</th>\n",
              "      <th>m3</th>\n",
              "      <th>m4</th>\n",
              "      <th>company0</th>\n",
              "      <th>company1</th>\n",
              "      <th>company2</th>\n",
              "      <th>segment</th>\n",
              "      <th>trend</th>\n",
              "      <th>stock-price</th>\n",
              "      <th>price increase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1878.000000</td>\n",
              "      <td>1878.000000</td>\n",
              "      <td>1878.000000</td>\n",
              "      <td>1878.000000</td>\n",
              "      <td>1878.000000</td>\n",
              "      <td>1878.000000</td>\n",
              "      <td>1878.000000</td>\n",
              "      <td>1878.000000</td>\n",
              "      <td>1878.000000</td>\n",
              "      <td>1878.000000</td>\n",
              "      <td>1878.000000</td>\n",
              "      <td>1878.000000</td>\n",
              "      <td>1878.000000</td>\n",
              "      <td>1878.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.202343</td>\n",
              "      <td>0.237487</td>\n",
              "      <td>0.341267</td>\n",
              "      <td>0.497734</td>\n",
              "      <td>0.499228</td>\n",
              "      <td>0.508413</td>\n",
              "      <td>0.053248</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.412939</td>\n",
              "      <td>0.555296</td>\n",
              "      <td>0.338658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.401854</td>\n",
              "      <td>0.425656</td>\n",
              "      <td>0.325503</td>\n",
              "      <td>0.286360</td>\n",
              "      <td>0.291111</td>\n",
              "      <td>0.297416</td>\n",
              "      <td>0.224588</td>\n",
              "      <td>0.471530</td>\n",
              "      <td>0.471530</td>\n",
              "      <td>0.471530</td>\n",
              "      <td>0.471530</td>\n",
              "      <td>0.399797</td>\n",
              "      <td>0.331809</td>\n",
              "      <td>0.473379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.245000</td>\n",
              "      <td>0.241095</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.263150</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.499700</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.649711</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.748750</td>\n",
              "      <td>0.754928</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.850723</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           expert1      expert2  ...  stock-price  price increase\n",
              "count  1878.000000  1878.000000  ...  1878.000000     1878.000000\n",
              "mean      0.202343     0.237487  ...     0.555296        0.338658\n",
              "std       0.401854     0.425656  ...     0.331809        0.473379\n",
              "min       0.000000     0.000000  ...     0.000000        0.000000\n",
              "25%       0.000000     0.000000  ...     0.263150        0.000000\n",
              "50%       0.000000     0.000000  ...     0.649711        0.000000\n",
              "75%       0.000000     0.000000  ...     0.850723        1.000000\n",
              "max       1.000000     1.000000  ...     1.000000        1.000000\n",
              "\n",
              "[8 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 346
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jesx5fwmYeYi"
      },
      "source": [
        "# Prepare data for training/testing\n",
        "\n",
        "Split the data in a training and test set. It is important that this is done randomly, as the data is divided per company (the first 1/3rd of the rows are only about company 0). Sklearn takes care of this in the method `train_test_split`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iq2kqk8vUp-a"
      },
      "source": [
        "#split training in training and test data\n",
        "columns = df_norm.columns.values.tolist()\n",
        "independent = columns[:-1]\n",
        "dependent = columns[-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_norm[independent],df_norm[dependent], test_size=0.2)"
      ],
      "execution_count": 347,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw1WsSIJXF-P"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "X = torch.tensor(X_train.values, dtype=torch.float)\n",
        "y = torch.tensor(y_train.values, dtype=torch.int64)\n",
        "\n",
        "train_ds = torch.utils.data.TensorDataset(X,y)\n",
        "\n",
        "X = torch.tensor(X_test.values, dtype=torch.float)\n",
        "y = torch.tensor(y_test.values, dtype=torch.int64)\n",
        "\n",
        "test_ds = torch.utils.data.TensorDataset(X,y)"
      ],
      "execution_count": 348,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDnqxFfndBE4"
      },
      "source": [
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE,shuffle=True)\n",
        "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 349,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vL6AWATbFkz"
      },
      "source": [
        "# Building the network\n",
        "\n",
        "Yes, this is what it is about"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I2mvoBOa9Sg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0f70af4-13eb-436a-ce0c-6079bd6d0b9c"
      },
      "source": [
        "# Build the network\n",
        "class stockANN(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(13,32)\n",
        "    self.av1 = nn.ReLU()\n",
        "\n",
        "    self.fc2 = nn.Linear(32,32) \n",
        "    self.fc2_bn = nn.BatchNorm1d(32)\n",
        "    #self.fc2_dp = nn.Dropout(p=0.2)\n",
        "    self.av2 = nn.ReLU()\n",
        "\n",
        "    self.fc3 = nn.Linear(32,32)\n",
        "    self.fc3_bn = nn.BatchNorm1d(32)\n",
        "    #self.fc3_dp = nn.Dropout(p=0.2)\n",
        "    self.av3 = nn.ReLU()\n",
        "\n",
        "    self.fc4 = nn.Linear(32,32)\n",
        "    self.fc4_bn = nn.BatchNorm1d(32)\n",
        "    self.av4 = nn.ReLU()\n",
        "\n",
        "    self.out = nn.Linear(32,1)\n",
        "    self.out_act = nn.Sigmoid()\n",
        "    return\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.av1(x)\n",
        "\n",
        "    x = self.fc2(x)\n",
        "    x = self.fc2_bn(x)\n",
        "    #x = self.fc2_dp(x)\n",
        "    x = self.av2(x)\n",
        "\n",
        "    x = self.fc3(x)\n",
        "    x = self.fc3_bn(x)\n",
        "    #x = self.fc2_dp(x)\n",
        "    x = self.av3(x)\n",
        "\n",
        "    x = self.fc4(x)\n",
        "    x = self.fc4_bn(x)\n",
        "    x = self.av4(x)\n",
        "\n",
        "    x = self.out(x)\n",
        "    y = self.out_act(x)\n",
        "    return y\n",
        "\n",
        "net = stockANN()\n",
        "print(net)"
      ],
      "execution_count": 350,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stockANN(\n",
            "  (fc1): Linear(in_features=13, out_features=32, bias=True)\n",
            "  (av1): ReLU()\n",
            "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc2_bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2_dp): Dropout(p=0.2, inplace=False)\n",
            "  (av2): ReLU()\n",
            "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc3_bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (av3): ReLU()\n",
            "  (fc4): Linear(in_features=32, out_features=32, bias=True)\n",
            "  (fc4_bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (av4): ReLU()\n",
            "  (out): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (out_act): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUmkURbxbeKg"
      },
      "source": [
        "# Training the network\n",
        "\n",
        "Train the deep learning network, this code is re-used from the labs.\n",
        "\n",
        "The loss function is changed to accomodate for the binary output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX4uaEJUbry-"
      },
      "source": [
        "# Train the network.\n",
        "\n",
        "# Note that you would first need to run the test code cells below\n",
        "\n",
        "def train():\n",
        "  \n",
        "  criterion = nn.BCELoss()\n",
        "\n",
        "  training_acc = []\n",
        "  test_acc = []\n",
        "  loss_table = []\n",
        "\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=0.0005)\n",
        "  for epocs in range(300):\n",
        "    losses = []\n",
        "    for data in train_dl:\n",
        "      X, y = data\n",
        "      optimizer.zero_grad() # Clear the gradients, before next batch.\n",
        "      output = net(X.view(-1,13))  # Forward pass\n",
        "    \n",
        "      y = y.unsqueeze(1) #                                          >>> THIS SOLVED THE ISSUE BELOW, SEEMS A BIT DIRTY <<<\n",
        "      y = y.float()\n",
        "    \n",
        "      loss = criterion(output, y) # Computing loss.                  >>> STUCK HERE, TENSOR DIMENSIONS <<<\n",
        "    \n",
        "      loss.backward()  # Back-propagation (computing gradients)\n",
        "    \n",
        "      losses.append(loss.data.numpy()) # For making the plot\n",
        "    \n",
        "      optimizer.step(); # Update the weights (using gradients). \n",
        "      # TODO, look at this indent, do you want to update the weights after every datapoint?\n",
        "\n",
        "    print('LOSS 1:',sum(losses)/len(losses))\n",
        "    print('LOSS 2:    ',loss.data)\n",
        "    print('TEST:    ',test_accuracy())\n",
        "    print('TRAINING:',training_accuracy())\n",
        "\n",
        "    test_acc+=[test_accuracy()]\n",
        "    training_acc+=[training_accuracy()]\n",
        "    loss_table+=[sum(losses)/len(losses)]\n",
        "\n",
        "  #print(e_losses,t_losses)\n",
        "  return training_acc,test_acc,loss_table\n"
      ],
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XBJ6P7RJKZB",
        "outputId": "4ba78d7b-a4ec-401b-fc97-1c9722fc851d"
      },
      "source": [
        "train_acc,test_acc,train_loss = train()"
      ],
      "execution_count": 355,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOSS 1: 0.4863423413418709\n",
            "LOSS 2:     tensor(0.4932)\n",
            "TEST:     0.809\n",
            "TRAINING: 0.838\n",
            "LOSS 1: 0.4078807875196985\n",
            "LOSS 2:     tensor(0.4220)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.861\n",
            "LOSS 1: 0.3614504229515157\n",
            "LOSS 2:     tensor(0.3352)\n",
            "TEST:     0.822\n",
            "TRAINING: 0.861\n",
            "LOSS 1: 0.3424724283370566\n",
            "LOSS 2:     tensor(0.2950)\n",
            "TEST:     0.822\n",
            "TRAINING: 0.858\n",
            "LOSS 1: 0.3290794076437646\n",
            "LOSS 2:     tensor(0.2327)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.858\n",
            "LOSS 1: 0.32824701958514274\n",
            "LOSS 2:     tensor(0.2815)\n",
            "TEST:     0.851\n",
            "TRAINING: 0.87\n",
            "LOSS 1: 0.321297139880505\n",
            "LOSS 2:     tensor(0.3364)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.862\n",
            "LOSS 1: 0.3111469396885405\n",
            "LOSS 2:     tensor(0.2800)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.871\n",
            "LOSS 1: 0.3072702231559348\n",
            "LOSS 2:     tensor(0.3118)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.874\n",
            "LOSS 1: 0.3048457202125103\n",
            "LOSS 2:     tensor(0.2704)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.867\n",
            "LOSS 1: 0.2961019017594926\n",
            "LOSS 2:     tensor(0.1874)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.864\n",
            "LOSS 1: 0.2993120310154367\n",
            "LOSS 2:     tensor(0.2805)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.878\n",
            "LOSS 1: 0.29329977358909365\n",
            "LOSS 2:     tensor(0.2708)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.87\n",
            "LOSS 1: 0.2915293019502721\n",
            "LOSS 2:     tensor(0.3458)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.877\n",
            "LOSS 1: 0.2888463282204689\n",
            "LOSS 2:     tensor(0.2168)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.876\n",
            "LOSS 1: 0.29602257558640016\n",
            "LOSS 2:     tensor(0.3223)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.864\n",
            "LOSS 1: 0.2891507820880159\n",
            "LOSS 2:     tensor(0.3289)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.868\n",
            "LOSS 1: 0.28413872072037233\n",
            "LOSS 2:     tensor(0.1508)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.874\n",
            "LOSS 1: 0.293071984927705\n",
            "LOSS 2:     tensor(0.4174)\n",
            "TEST:     0.846\n",
            "TRAINING: 0.872\n",
            "LOSS 1: 0.28476492553315264\n",
            "LOSS 2:     tensor(0.3979)\n",
            "TEST:     0.846\n",
            "TRAINING: 0.874\n",
            "LOSS 1: 0.28537290432351703\n",
            "LOSS 2:     tensor(0.2144)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.87\n",
            "LOSS 1: 0.2723515122494799\n",
            "LOSS 2:     tensor(0.1961)\n",
            "TEST:     0.846\n",
            "TRAINING: 0.874\n",
            "LOSS 1: 0.28146410971245867\n",
            "LOSS 2:     tensor(0.2446)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.885\n",
            "LOSS 1: 0.26676439351223885\n",
            "LOSS 2:     tensor(0.1600)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.873\n",
            "LOSS 1: 0.28681172113469305\n",
            "LOSS 2:     tensor(0.3189)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.877\n",
            "LOSS 1: 0.27092259377241135\n",
            "LOSS 2:     tensor(0.2699)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.877\n",
            "LOSS 1: 0.26647498347657794\n",
            "LOSS 2:     tensor(0.1598)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.878\n",
            "LOSS 1: 0.26541376050482407\n",
            "LOSS 2:     tensor(0.2479)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.881\n",
            "LOSS 1: 0.2714295333370249\n",
            "LOSS 2:     tensor(0.2752)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.884\n",
            "LOSS 1: 0.27545522819174095\n",
            "LOSS 2:     tensor(0.3313)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.889\n",
            "LOSS 1: 0.2673430690105925\n",
            "LOSS 2:     tensor(0.2085)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.874\n",
            "LOSS 1: 0.25535768080264964\n",
            "LOSS 2:     tensor(0.2367)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.879\n",
            "LOSS 1: 0.2660030576776951\n",
            "LOSS 2:     tensor(0.1401)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.883\n",
            "LOSS 1: 0.25546972168252824\n",
            "LOSS 2:     tensor(0.2551)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.891\n",
            "LOSS 1: 0.2525737556688329\n",
            "LOSS 2:     tensor(0.1526)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.881\n",
            "LOSS 1: 0.2566120339200852\n",
            "LOSS 2:     tensor(0.1875)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.891\n",
            "LOSS 1: 0.2623532584055941\n",
            "LOSS 2:     tensor(0.2396)\n",
            "TEST:     0.819\n",
            "TRAINING: 0.877\n",
            "LOSS 1: 0.26198948650284015\n",
            "LOSS 2:     tensor(0.2725)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.883\n",
            "LOSS 1: 0.2578872523092209\n",
            "LOSS 2:     tensor(0.3749)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.888\n",
            "LOSS 1: 0.25860608020361436\n",
            "LOSS 2:     tensor(0.2498)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.891\n",
            "LOSS 1: 0.23994775655421804\n",
            "LOSS 2:     tensor(0.3093)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.887\n",
            "LOSS 1: 0.250165988790228\n",
            "LOSS 2:     tensor(0.2649)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.895\n",
            "LOSS 1: 0.24575635125028325\n",
            "LOSS 2:     tensor(0.1979)\n",
            "TEST:     0.809\n",
            "TRAINING: 0.894\n",
            "LOSS 1: 0.2508081666966702\n",
            "LOSS 2:     tensor(0.1578)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.895\n",
            "LOSS 1: 0.24628513734391394\n",
            "LOSS 2:     tensor(0.2932)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.89\n",
            "LOSS 1: 0.2353092243379735\n",
            "LOSS 2:     tensor(0.1554)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.893\n",
            "LOSS 1: 0.22990832509512596\n",
            "LOSS 2:     tensor(0.1684)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.891\n",
            "LOSS 1: 0.24050153078550987\n",
            "LOSS 2:     tensor(0.1732)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.883\n",
            "LOSS 1: 0.2325668862842499\n",
            "LOSS 2:     tensor(0.2363)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.895\n",
            "LOSS 1: 0.23645161980010093\n",
            "LOSS 2:     tensor(0.1152)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.881\n",
            "LOSS 1: 0.22978800535202026\n",
            "LOSS 2:     tensor(0.2530)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.885\n",
            "LOSS 1: 0.23308661167925976\n",
            "LOSS 2:     tensor(0.1473)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.89\n",
            "LOSS 1: 0.2452711540650814\n",
            "LOSS 2:     tensor(0.2819)\n",
            "TEST:     0.822\n",
            "TRAINING: 0.895\n",
            "LOSS 1: 0.23330915370520125\n",
            "LOSS 2:     tensor(0.1654)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.886\n",
            "LOSS 1: 0.23259447673533826\n",
            "LOSS 2:     tensor(0.1860)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.897\n",
            "LOSS 1: 0.2253814697899717\n",
            "LOSS 2:     tensor(0.1914)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.899\n",
            "LOSS 1: 0.2421820969023603\n",
            "LOSS 2:     tensor(0.1676)\n",
            "TEST:     0.848\n",
            "TRAINING: 0.887\n",
            "LOSS 1: 0.22515363737623742\n",
            "LOSS 2:     tensor(0.1630)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.903\n",
            "LOSS 1: 0.2292530080105396\n",
            "LOSS 2:     tensor(0.1253)\n",
            "TEST:     0.846\n",
            "TRAINING: 0.899\n",
            "LOSS 1: 0.23109329365035322\n",
            "LOSS 2:     tensor(0.1857)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.884\n",
            "LOSS 1: 0.22729113365107395\n",
            "LOSS 2:     tensor(0.1867)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.892\n",
            "LOSS 1: 0.23127114313079955\n",
            "LOSS 2:     tensor(0.2691)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.898\n",
            "LOSS 1: 0.21284704909045646\n",
            "LOSS 2:     tensor(0.1388)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.897\n",
            "LOSS 1: 0.22349146167014508\n",
            "LOSS 2:     tensor(0.2595)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.896\n",
            "LOSS 1: 0.21755821416352658\n",
            "LOSS 2:     tensor(0.1822)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.902\n",
            "LOSS 1: 0.22577877374405556\n",
            "LOSS 2:     tensor(0.5348)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.894\n",
            "LOSS 1: 0.21782530352790305\n",
            "LOSS 2:     tensor(0.2221)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.908\n",
            "LOSS 1: 0.21779952404346872\n",
            "LOSS 2:     tensor(0.1954)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.906\n",
            "LOSS 1: 0.21304627079912958\n",
            "LOSS 2:     tensor(0.2058)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.903\n",
            "LOSS 1: 0.21794332928479987\n",
            "LOSS 2:     tensor(0.3104)\n",
            "TEST:     0.848\n",
            "TRAINING: 0.902\n",
            "LOSS 1: 0.21297427536325253\n",
            "LOSS 2:     tensor(0.1922)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.914\n",
            "LOSS 1: 0.20311231181976644\n",
            "LOSS 2:     tensor(0.1732)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.911\n",
            "LOSS 1: 0.21432144321659777\n",
            "LOSS 2:     tensor(0.2422)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.905\n",
            "LOSS 1: 0.21288898587226868\n",
            "LOSS 2:     tensor(0.1403)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.903\n",
            "LOSS 1: 0.20870232582092285\n",
            "LOSS 2:     tensor(0.3602)\n",
            "TEST:     0.846\n",
            "TRAINING: 0.903\n",
            "LOSS 1: 0.20453715736561634\n",
            "LOSS 2:     tensor(0.3858)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.91\n",
            "LOSS 1: 0.2192151221506139\n",
            "LOSS 2:     tensor(0.2464)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.902\n",
            "LOSS 1: 0.21503329435561566\n",
            "LOSS 2:     tensor(0.2316)\n",
            "TEST:     0.814\n",
            "TRAINING: 0.907\n",
            "LOSS 1: 0.20620913280451553\n",
            "LOSS 2:     tensor(0.1323)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.903\n",
            "LOSS 1: 0.1993673246433126\n",
            "LOSS 2:     tensor(0.0484)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.911\n",
            "LOSS 1: 0.2032441111638191\n",
            "LOSS 2:     tensor(0.1895)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.903\n",
            "LOSS 1: 0.19462711474996933\n",
            "LOSS 2:     tensor(0.2446)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.913\n",
            "LOSS 1: 0.20027119254178188\n",
            "LOSS 2:     tensor(0.1481)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.915\n",
            "LOSS 1: 0.1861845055159102\n",
            "LOSS 2:     tensor(0.1308)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.91\n",
            "LOSS 1: 0.20189266731130315\n",
            "LOSS 2:     tensor(0.1190)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.914\n",
            "LOSS 1: 0.21025241213910123\n",
            "LOSS 2:     tensor(0.2301)\n",
            "TEST:     0.819\n",
            "TRAINING: 0.911\n",
            "LOSS 1: 0.19178033620119095\n",
            "LOSS 2:     tensor(0.1958)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.907\n",
            "LOSS 1: 0.19274706441037198\n",
            "LOSS 2:     tensor(0.1802)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.911\n",
            "LOSS 1: 0.20244697639916806\n",
            "LOSS 2:     tensor(0.2960)\n",
            "TEST:     0.822\n",
            "TRAINING: 0.919\n",
            "LOSS 1: 0.20115653314489\n",
            "LOSS 2:     tensor(0.1110)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.909\n",
            "LOSS 1: 0.1982821834848282\n",
            "LOSS 2:     tensor(0.1905)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.921\n",
            "LOSS 1: 0.1962395744754913\n",
            "LOSS 2:     tensor(0.1784)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.913\n",
            "LOSS 1: 0.19475209364231597\n",
            "LOSS 2:     tensor(0.2026)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.915\n",
            "LOSS 1: 0.19108616045497834\n",
            "LOSS 2:     tensor(0.1635)\n",
            "TEST:     0.859\n",
            "TRAINING: 0.919\n",
            "LOSS 1: 0.18651511107987545\n",
            "LOSS 2:     tensor(0.2051)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.909\n",
            "LOSS 1: 0.18035065541242032\n",
            "LOSS 2:     tensor(0.2175)\n",
            "TEST:     0.848\n",
            "TRAINING: 0.928\n",
            "LOSS 1: 0.19712207101761026\n",
            "LOSS 2:     tensor(0.1181)\n",
            "TEST:     0.846\n",
            "TRAINING: 0.916\n",
            "LOSS 1: 0.1836531862933585\n",
            "LOSS 2:     tensor(0.1292)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.917\n",
            "LOSS 1: 0.18941437183542453\n",
            "LOSS 2:     tensor(0.3625)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.913\n",
            "LOSS 1: 0.19963181185278486\n",
            "LOSS 2:     tensor(0.0992)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.922\n",
            "LOSS 1: 0.18114950301799368\n",
            "LOSS 2:     tensor(0.1007)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.91\n",
            "LOSS 1: 0.19423323822148303\n",
            "LOSS 2:     tensor(0.3145)\n",
            "TEST:     0.846\n",
            "TRAINING: 0.92\n",
            "LOSS 1: 0.18079323765445263\n",
            "LOSS 2:     tensor(0.1738)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.904\n",
            "LOSS 1: 0.18325032246239642\n",
            "LOSS 2:     tensor(0.1375)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.92\n",
            "LOSS 1: 0.17720846166001988\n",
            "LOSS 2:     tensor(0.2363)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.928\n",
            "LOSS 1: 0.1737656594907984\n",
            "LOSS 2:     tensor(0.1183)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.927\n",
            "LOSS 1: 0.17491016790587852\n",
            "LOSS 2:     tensor(0.4737)\n",
            "TEST:     0.814\n",
            "TRAINING: 0.921\n",
            "LOSS 1: 0.18334252839075757\n",
            "LOSS 2:     tensor(0.2307)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.921\n",
            "LOSS 1: 0.17720293776786072\n",
            "LOSS 2:     tensor(0.0871)\n",
            "TEST:     0.846\n",
            "TRAINING: 0.919\n",
            "LOSS 1: 0.17853556177083482\n",
            "LOSS 2:     tensor(0.1629)\n",
            "TEST:     0.848\n",
            "TRAINING: 0.919\n",
            "LOSS 1: 0.1621346394432352\n",
            "LOSS 2:     tensor(0.1450)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.923\n",
            "LOSS 1: 0.188624861868138\n",
            "LOSS 2:     tensor(0.1322)\n",
            "TEST:     0.819\n",
            "TRAINING: 0.916\n",
            "LOSS 1: 0.17181143870062016\n",
            "LOSS 2:     tensor(0.0632)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.918\n",
            "LOSS 1: 0.1889222963376248\n",
            "LOSS 2:     tensor(0.1163)\n",
            "TEST:     0.848\n",
            "TRAINING: 0.933\n",
            "LOSS 1: 0.17837004775696613\n",
            "LOSS 2:     tensor(0.3346)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.924\n",
            "LOSS 1: 0.1708432299659607\n",
            "LOSS 2:     tensor(0.3048)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.926\n",
            "LOSS 1: 0.17544008078093223\n",
            "LOSS 2:     tensor(0.1231)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.931\n",
            "LOSS 1: 0.18023252740819404\n",
            "LOSS 2:     tensor(0.1700)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.931\n",
            "LOSS 1: 0.18329113167017064\n",
            "LOSS 2:     tensor(0.2467)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.923\n",
            "LOSS 1: 0.17120418681743296\n",
            "LOSS 2:     tensor(0.1727)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.929\n",
            "LOSS 1: 0.16714006036202958\n",
            "LOSS 2:     tensor(0.1096)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.928\n",
            "LOSS 1: 0.17748793809020774\n",
            "LOSS 2:     tensor(0.2242)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.937\n",
            "LOSS 1: 0.16648545988062594\n",
            "LOSS 2:     tensor(0.1141)\n",
            "TEST:     0.851\n",
            "TRAINING: 0.926\n",
            "LOSS 1: 0.15837015695077308\n",
            "LOSS 2:     tensor(0.1408)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.931\n",
            "LOSS 1: 0.1695816143554576\n",
            "LOSS 2:     tensor(0.1624)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.933\n",
            "LOSS 1: 0.19295287259081575\n",
            "LOSS 2:     tensor(0.1787)\n",
            "TEST:     0.819\n",
            "TRAINING: 0.915\n",
            "LOSS 1: 0.17138534800169317\n",
            "LOSS 2:     tensor(0.1209)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.93\n",
            "LOSS 1: 0.15659161149821382\n",
            "LOSS 2:     tensor(0.2232)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.938\n",
            "LOSS 1: 0.16829747421310304\n",
            "LOSS 2:     tensor(0.0938)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.929\n",
            "LOSS 1: 0.15889497315312953\n",
            "LOSS 2:     tensor(0.2096)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.927\n",
            "LOSS 1: 0.16304006443378774\n",
            "LOSS 2:     tensor(0.1555)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.932\n",
            "LOSS 1: 0.14187493365495762\n",
            "LOSS 2:     tensor(0.1368)\n",
            "TEST:     0.819\n",
            "TRAINING: 0.937\n",
            "LOSS 1: 0.15731311946156176\n",
            "LOSS 2:     tensor(0.0728)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.929\n",
            "LOSS 1: 0.1497682050821629\n",
            "LOSS 2:     tensor(0.0798)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.927\n",
            "LOSS 1: 0.1729494612267677\n",
            "LOSS 2:     tensor(0.0967)\n",
            "TEST:     0.819\n",
            "TRAINING: 0.916\n",
            "LOSS 1: 0.15247562717884144\n",
            "LOSS 2:     tensor(0.2418)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.934\n",
            "LOSS 1: 0.1692896694579023\n",
            "LOSS 2:     tensor(0.1620)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.933\n",
            "LOSS 1: 0.1669875906502947\n",
            "LOSS 2:     tensor(0.1714)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.937\n",
            "LOSS 1: 0.15914446844699534\n",
            "LOSS 2:     tensor(0.1227)\n",
            "TEST:     0.811\n",
            "TRAINING: 0.934\n",
            "LOSS 1: 0.15296352107791192\n",
            "LOSS 2:     tensor(0.1429)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.939\n",
            "LOSS 1: 0.16876469584221535\n",
            "LOSS 2:     tensor(0.2427)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.929\n",
            "LOSS 1: 0.1607768272465848\n",
            "LOSS 2:     tensor(0.2074)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.933\n",
            "LOSS 1: 0.14816773754168064\n",
            "LOSS 2:     tensor(0.0774)\n",
            "TEST:     0.846\n",
            "TRAINING: 0.935\n",
            "LOSS 1: 0.15502076508834006\n",
            "LOSS 2:     tensor(0.1213)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.939\n",
            "LOSS 1: 0.1606043479702574\n",
            "LOSS 2:     tensor(0.1029)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.938\n",
            "LOSS 1: 0.14131631718036977\n",
            "LOSS 2:     tensor(0.1397)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.939\n",
            "LOSS 1: 0.14626021112533324\n",
            "LOSS 2:     tensor(0.1965)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.935\n",
            "LOSS 1: 0.15114973414134472\n",
            "LOSS 2:     tensor(0.1165)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.941\n",
            "LOSS 1: 0.14112777350113748\n",
            "LOSS 2:     tensor(0.1417)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.944\n",
            "LOSS 1: 0.1466479000258953\n",
            "LOSS 2:     tensor(0.1018)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.937\n",
            "LOSS 1: 0.1513896220700538\n",
            "LOSS 2:     tensor(0.3444)\n",
            "TEST:     0.854\n",
            "TRAINING: 0.931\n",
            "LOSS 1: 0.15838565265244625\n",
            "LOSS 2:     tensor(0.1847)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.931\n",
            "LOSS 1: 0.15277494165174504\n",
            "LOSS 2:     tensor(0.1793)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.941\n",
            "LOSS 1: 0.15603127615882995\n",
            "LOSS 2:     tensor(0.1887)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.953\n",
            "LOSS 1: 0.15789118726202783\n",
            "LOSS 2:     tensor(0.2126)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.95\n",
            "LOSS 1: 0.13637856560501646\n",
            "LOSS 2:     tensor(0.0811)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.941\n",
            "LOSS 1: 0.1379406772395398\n",
            "LOSS 2:     tensor(0.2584)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.943\n",
            "LOSS 1: 0.1680026614761099\n",
            "LOSS 2:     tensor(0.1477)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.941\n",
            "LOSS 1: 0.14761025953958642\n",
            "LOSS 2:     tensor(0.2365)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.941\n",
            "LOSS 1: 0.16159742491993498\n",
            "LOSS 2:     tensor(0.2645)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.931\n",
            "LOSS 1: 0.14391745048317503\n",
            "LOSS 2:     tensor(0.1325)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.937\n",
            "LOSS 1: 0.16474070859716294\n",
            "LOSS 2:     tensor(0.1375)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.941\n",
            "LOSS 1: 0.14052523268347092\n",
            "LOSS 2:     tensor(0.2007)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.941\n",
            "LOSS 1: 0.14957936960173415\n",
            "LOSS 2:     tensor(0.0750)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.948\n",
            "LOSS 1: 0.1531911700012836\n",
            "LOSS 2:     tensor(0.0789)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.933\n",
            "LOSS 1: 0.14851937530205606\n",
            "LOSS 2:     tensor(0.1206)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.943\n",
            "LOSS 1: 0.14760252080382186\n",
            "LOSS 2:     tensor(0.1892)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.94\n",
            "LOSS 1: 0.14414236012925494\n",
            "LOSS 2:     tensor(0.1788)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.953\n",
            "LOSS 1: 0.15199118384972532\n",
            "LOSS 2:     tensor(0.1678)\n",
            "TEST:     0.848\n",
            "TRAINING: 0.937\n",
            "LOSS 1: 0.16540682712133895\n",
            "LOSS 2:     tensor(0.1976)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.943\n",
            "LOSS 1: 0.12698487683813622\n",
            "LOSS 2:     tensor(0.1240)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.955\n",
            "LOSS 1: 0.1375926060879484\n",
            "LOSS 2:     tensor(0.0768)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.946\n",
            "LOSS 1: 0.14303198211053583\n",
            "LOSS 2:     tensor(0.0970)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.955\n",
            "LOSS 1: 0.14478200995066065\n",
            "LOSS 2:     tensor(0.0778)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.941\n",
            "LOSS 1: 0.15087106919034998\n",
            "LOSS 2:     tensor(0.2234)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.944\n",
            "LOSS 1: 0.14427894781878653\n",
            "LOSS 2:     tensor(0.0789)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.94\n",
            "LOSS 1: 0.14726713355234328\n",
            "LOSS 2:     tensor(0.1251)\n",
            "TEST:     0.848\n",
            "TRAINING: 0.943\n",
            "LOSS 1: 0.14082601721933546\n",
            "LOSS 2:     tensor(0.1698)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.945\n",
            "LOSS 1: 0.13189335967949095\n",
            "LOSS 2:     tensor(0.1123)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.95\n",
            "LOSS 1: 0.12786923230011413\n",
            "LOSS 2:     tensor(0.1410)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.941\n",
            "LOSS 1: 0.1311117204579901\n",
            "LOSS 2:     tensor(0.2392)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.937\n",
            "LOSS 1: 0.1273594022272749\n",
            "LOSS 2:     tensor(0.1899)\n",
            "TEST:     0.848\n",
            "TRAINING: 0.949\n",
            "LOSS 1: 0.13832261995907794\n",
            "LOSS 2:     tensor(0.1244)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.946\n",
            "LOSS 1: 0.12284451477388118\n",
            "LOSS 2:     tensor(0.1401)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.947\n",
            "LOSS 1: 0.12389792125434318\n",
            "LOSS 2:     tensor(0.2024)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.953\n",
            "LOSS 1: 0.14084356817159246\n",
            "LOSS 2:     tensor(0.1957)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.941\n",
            "LOSS 1: 0.13436687816964818\n",
            "LOSS 2:     tensor(0.1244)\n",
            "TEST:     0.846\n",
            "TRAINING: 0.957\n",
            "LOSS 1: 0.1373360091225898\n",
            "LOSS 2:     tensor(0.0495)\n",
            "TEST:     0.848\n",
            "TRAINING: 0.949\n",
            "LOSS 1: 0.1279829169048908\n",
            "LOSS 2:     tensor(0.1641)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.943\n",
            "LOSS 1: 0.11949706565033882\n",
            "LOSS 2:     tensor(0.0510)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.945\n",
            "LOSS 1: 0.13465639168119176\n",
            "LOSS 2:     tensor(0.1069)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.948\n",
            "LOSS 1: 0.12680793720039915\n",
            "LOSS 2:     tensor(0.1890)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.949\n",
            "LOSS 1: 0.1331070120981399\n",
            "LOSS 2:     tensor(0.1477)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.956\n",
            "LOSS 1: 0.12223223961414174\n",
            "LOSS 2:     tensor(0.0773)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.955\n",
            "LOSS 1: 0.12889371404146893\n",
            "LOSS 2:     tensor(0.2642)\n",
            "TEST:     0.822\n",
            "TRAINING: 0.937\n",
            "LOSS 1: 0.1303971896463252\n",
            "LOSS 2:     tensor(0.0427)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.953\n",
            "LOSS 1: 0.13661546196709287\n",
            "LOSS 2:     tensor(0.1630)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.951\n",
            "LOSS 1: 0.1510436457205326\n",
            "LOSS 2:     tensor(0.1592)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.94\n",
            "LOSS 1: 0.12757356956284097\n",
            "LOSS 2:     tensor(0.2145)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.95\n",
            "LOSS 1: 0.11881213108117276\n",
            "LOSS 2:     tensor(0.1949)\n",
            "TEST:     0.848\n",
            "TRAINING: 0.953\n",
            "LOSS 1: 0.1313181951087206\n",
            "LOSS 2:     tensor(0.1605)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.959\n",
            "LOSS 1: 0.11661963144990992\n",
            "LOSS 2:     tensor(0.1944)\n",
            "TEST:     0.846\n",
            "TRAINING: 0.955\n",
            "LOSS 1: 0.1276295153384513\n",
            "LOSS 2:     tensor(0.0359)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.938\n",
            "LOSS 1: 0.12956545668396544\n",
            "LOSS 2:     tensor(0.0734)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.956\n",
            "LOSS 1: 0.1208480808170552\n",
            "LOSS 2:     tensor(0.0769)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.947\n",
            "LOSS 1: 0.12313147626341657\n",
            "LOSS 2:     tensor(0.2357)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.952\n",
            "LOSS 1: 0.14129100347611498\n",
            "LOSS 2:     tensor(0.1486)\n",
            "TEST:     0.819\n",
            "TRAINING: 0.955\n",
            "LOSS 1: 0.12836497514805895\n",
            "LOSS 2:     tensor(0.0462)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.95\n",
            "LOSS 1: 0.12668396912990731\n",
            "LOSS 2:     tensor(0.2201)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.947\n",
            "LOSS 1: 0.1163712555265173\n",
            "LOSS 2:     tensor(0.1182)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.957\n",
            "LOSS 1: 0.12140043301785246\n",
            "LOSS 2:     tensor(0.0855)\n",
            "TEST:     0.848\n",
            "TRAINING: 0.951\n",
            "LOSS 1: 0.12576261951409756\n",
            "LOSS 2:     tensor(0.2706)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.951\n",
            "LOSS 1: 0.1238348460260858\n",
            "LOSS 2:     tensor(0.2133)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.958\n",
            "LOSS 1: 0.13247445400090926\n",
            "LOSS 2:     tensor(0.0785)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.954\n",
            "LOSS 1: 0.13173644419046157\n",
            "LOSS 2:     tensor(0.1259)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.951\n",
            "LOSS 1: 0.11533612646955124\n",
            "LOSS 2:     tensor(0.0947)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.957\n",
            "LOSS 1: 0.12768386847319754\n",
            "LOSS 2:     tensor(0.3059)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.955\n",
            "LOSS 1: 0.1115783784300723\n",
            "LOSS 2:     tensor(0.1265)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.952\n",
            "LOSS 1: 0.1285324112928294\n",
            "LOSS 2:     tensor(0.1465)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.954\n",
            "LOSS 1: 0.12147949913397749\n",
            "LOSS 2:     tensor(0.1976)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.956\n",
            "LOSS 1: 0.12751281047438054\n",
            "LOSS 2:     tensor(0.1078)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.957\n",
            "LOSS 1: 0.12354499240346412\n",
            "LOSS 2:     tensor(0.1158)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.961\n",
            "LOSS 1: 0.11847992835843817\n",
            "LOSS 2:     tensor(0.1029)\n",
            "TEST:     0.819\n",
            "TRAINING: 0.961\n",
            "LOSS 1: 0.12774975419520063\n",
            "LOSS 2:     tensor(0.1107)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.952\n",
            "LOSS 1: 0.12212335528369914\n",
            "LOSS 2:     tensor(0.0791)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.951\n",
            "LOSS 1: 0.1366182805851419\n",
            "LOSS 2:     tensor(0.0611)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.952\n",
            "LOSS 1: 0.11564831281120473\n",
            "LOSS 2:     tensor(0.0855)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.958\n",
            "LOSS 1: 0.1181661623748059\n",
            "LOSS 2:     tensor(0.1290)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.955\n",
            "LOSS 1: 0.1141592916299371\n",
            "LOSS 2:     tensor(0.1732)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.952\n",
            "LOSS 1: 0.10500999872988843\n",
            "LOSS 2:     tensor(0.0981)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.957\n",
            "LOSS 1: 0.10540775384040589\n",
            "LOSS 2:     tensor(0.1098)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.957\n",
            "LOSS 1: 0.13847153126559358\n",
            "LOSS 2:     tensor(0.1521)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.943\n",
            "LOSS 1: 0.1043975238311798\n",
            "LOSS 2:     tensor(0.0400)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.962\n",
            "LOSS 1: 0.11652364250906605\n",
            "LOSS 2:     tensor(0.2109)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.952\n",
            "LOSS 1: 0.12311457890145322\n",
            "LOSS 2:     tensor(0.1146)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.959\n",
            "LOSS 1: 0.11148640640238498\n",
            "LOSS 2:     tensor(0.0564)\n",
            "TEST:     0.824\n",
            "TRAINING: 0.957\n",
            "LOSS 1: 0.11161233770086411\n",
            "LOSS 2:     tensor(0.0772)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.957\n",
            "LOSS 1: 0.12342616226127807\n",
            "LOSS 2:     tensor(0.1876)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.957\n",
            "LOSS 1: 0.11568268642146537\n",
            "LOSS 2:     tensor(0.1204)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.965\n",
            "LOSS 1: 0.121426116120308\n",
            "LOSS 2:     tensor(0.0940)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.956\n",
            "LOSS 1: 0.11822759819791671\n",
            "LOSS 2:     tensor(0.0949)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.963\n",
            "LOSS 1: 0.113840293654419\n",
            "LOSS 2:     tensor(0.1618)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.96\n",
            "LOSS 1: 0.11003821446223462\n",
            "LOSS 2:     tensor(0.0536)\n",
            "TEST:     0.854\n",
            "TRAINING: 0.955\n",
            "LOSS 1: 0.13335946463841072\n",
            "LOSS 2:     tensor(0.1650)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.957\n",
            "LOSS 1: 0.1156448291892067\n",
            "LOSS 2:     tensor(0.0581)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.943\n",
            "LOSS 1: 0.11485947157315751\n",
            "LOSS 2:     tensor(0.3004)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.957\n",
            "LOSS 1: 0.11487499351038578\n",
            "LOSS 2:     tensor(0.0961)\n",
            "TEST:     0.846\n",
            "TRAINING: 0.946\n",
            "LOSS 1: 0.09981226762558551\n",
            "LOSS 2:     tensor(0.0822)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.955\n",
            "LOSS 1: 0.1071253840989889\n",
            "LOSS 2:     tensor(0.1212)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.959\n",
            "LOSS 1: 0.12736110064260503\n",
            "LOSS 2:     tensor(0.0862)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.955\n",
            "LOSS 1: 0.1293705720058147\n",
            "LOSS 2:     tensor(0.1064)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.952\n",
            "LOSS 1: 0.10946682424462856\n",
            "LOSS 2:     tensor(0.0790)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.957\n",
            "LOSS 1: 0.11371330322420344\n",
            "LOSS 2:     tensor(0.1609)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.961\n",
            "LOSS 1: 0.11453350542232077\n",
            "LOSS 2:     tensor(0.1731)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.949\n",
            "LOSS 1: 0.10902738059930345\n",
            "LOSS 2:     tensor(0.0826)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.966\n",
            "LOSS 1: 0.11776623312146106\n",
            "LOSS 2:     tensor(0.0462)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.957\n",
            "LOSS 1: 0.10585296306600596\n",
            "LOSS 2:     tensor(0.0547)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.955\n",
            "LOSS 1: 0.1121413592407678\n",
            "LOSS 2:     tensor(0.0560)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.964\n",
            "LOSS 1: 0.10612855554419628\n",
            "LOSS 2:     tensor(0.1391)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.963\n",
            "LOSS 1: 0.09851652974302465\n",
            "LOSS 2:     tensor(0.1352)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.961\n",
            "LOSS 1: 0.11953844189485337\n",
            "LOSS 2:     tensor(0.0746)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.961\n",
            "LOSS 1: 0.09973144144850209\n",
            "LOSS 2:     tensor(0.0571)\n",
            "TEST:     0.822\n",
            "TRAINING: 0.962\n",
            "LOSS 1: 0.10075940861505397\n",
            "LOSS 2:     tensor(0.2027)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.965\n",
            "LOSS 1: 0.11483835979820566\n",
            "LOSS 2:     tensor(0.0273)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.962\n",
            "LOSS 1: 0.10005709861821317\n",
            "LOSS 2:     tensor(0.0605)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.956\n",
            "LOSS 1: 0.09889805485355727\n",
            "LOSS 2:     tensor(0.0085)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.962\n",
            "LOSS 1: 0.10793074997181588\n",
            "LOSS 2:     tensor(0.0458)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.967\n",
            "LOSS 1: 0.10899246159069081\n",
            "LOSS 2:     tensor(0.0764)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.965\n",
            "LOSS 1: 0.10692534112232796\n",
            "LOSS 2:     tensor(0.1412)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.963\n",
            "LOSS 1: 0.11259879970407867\n",
            "LOSS 2:     tensor(0.1026)\n",
            "TEST:     0.846\n",
            "TRAINING: 0.963\n",
            "LOSS 1: 0.10198028330156143\n",
            "LOSS 2:     tensor(0.2769)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.955\n",
            "LOSS 1: 0.11394967602446993\n",
            "LOSS 2:     tensor(0.0907)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.959\n",
            "LOSS 1: 0.1015038695582684\n",
            "LOSS 2:     tensor(0.1901)\n",
            "TEST:     0.848\n",
            "TRAINING: 0.965\n",
            "LOSS 1: 0.10150450565158686\n",
            "LOSS 2:     tensor(0.1174)\n",
            "TEST:     0.848\n",
            "TRAINING: 0.947\n",
            "LOSS 1: 0.1135518392547965\n",
            "LOSS 2:     tensor(0.0520)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.967\n",
            "LOSS 1: 0.10304093634353038\n",
            "LOSS 2:     tensor(0.1406)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.959\n",
            "LOSS 1: 0.11690183249401286\n",
            "LOSS 2:     tensor(0.0752)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.955\n",
            "LOSS 1: 0.10891860461932548\n",
            "LOSS 2:     tensor(0.0736)\n",
            "TEST:     0.848\n",
            "TRAINING: 0.956\n",
            "LOSS 1: 0.11658721191293382\n",
            "LOSS 2:     tensor(0.1684)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.964\n",
            "LOSS 1: 0.10127214794146254\n",
            "LOSS 2:     tensor(0.0456)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.96\n",
            "LOSS 1: 0.10274817330881636\n",
            "LOSS 2:     tensor(0.1195)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.957\n",
            "LOSS 1: 0.10709571301382273\n",
            "LOSS 2:     tensor(0.1116)\n",
            "TEST:     0.851\n",
            "TRAINING: 0.964\n",
            "LOSS 1: 0.09851144448398276\n",
            "LOSS 2:     tensor(0.1470)\n",
            "TEST:     0.854\n",
            "TRAINING: 0.969\n",
            "LOSS 1: 0.10943475151632695\n",
            "LOSS 2:     tensor(0.1211)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.963\n",
            "LOSS 1: 0.10593535069455491\n",
            "LOSS 2:     tensor(0.0751)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.968\n",
            "LOSS 1: 0.10133346150371622\n",
            "LOSS 2:     tensor(0.1059)\n",
            "TEST:     0.832\n",
            "TRAINING: 0.96\n",
            "LOSS 1: 0.09908191049273343\n",
            "LOSS 2:     tensor(0.0739)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.956\n",
            "LOSS 1: 0.0998500229275607\n",
            "LOSS 2:     tensor(0.0888)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.961\n",
            "LOSS 1: 0.11016304473927681\n",
            "LOSS 2:     tensor(0.0676)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.96\n",
            "LOSS 1: 0.10084661988026285\n",
            "LOSS 2:     tensor(0.1219)\n",
            "TEST:     0.848\n",
            "TRAINING: 0.964\n",
            "LOSS 1: 0.12951244505003412\n",
            "LOSS 2:     tensor(0.0643)\n",
            "TEST:     0.83\n",
            "TRAINING: 0.951\n",
            "LOSS 1: 0.09563873815251157\n",
            "LOSS 2:     tensor(0.0943)\n",
            "TEST:     0.848\n",
            "TRAINING: 0.965\n",
            "LOSS 1: 0.09669031416799159\n",
            "LOSS 2:     tensor(0.0142)\n",
            "TEST:     0.827\n",
            "TRAINING: 0.962\n",
            "LOSS 1: 0.10273048230149645\n",
            "LOSS 2:     tensor(0.1591)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.96\n",
            "LOSS 1: 0.09993791623794018\n",
            "LOSS 2:     tensor(0.1403)\n",
            "TEST:     0.835\n",
            "TRAINING: 0.96\n",
            "LOSS 1: 0.1003548773600066\n",
            "LOSS 2:     tensor(0.1547)\n",
            "TEST:     0.838\n",
            "TRAINING: 0.962\n",
            "LOSS 1: 0.09529446346804182\n",
            "LOSS 2:     tensor(0.0560)\n",
            "TEST:     0.843\n",
            "TRAINING: 0.957\n",
            "LOSS 1: 0.10613345115029431\n",
            "LOSS 2:     tensor(0.0385)\n",
            "TEST:     0.856\n",
            "TRAINING: 0.969\n",
            "LOSS 1: 0.11188852573011784\n",
            "LOSS 2:     tensor(0.1017)\n",
            "TEST:     0.84\n",
            "TRAINING: 0.969\n",
            "LOSS 1: 0.0849905343765908\n",
            "LOSS 2:     tensor(0.2816)\n",
            "TEST:     0.854\n",
            "TRAINING: 0.965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "d7cDHIrdJLNP",
        "outputId": "9f2f608d-39da-421c-88af-ef9991ffe837"
      },
      "source": [
        "plt.plot(train_acc, label='TRAINING ACCURACY')\n",
        "plt.plot(test_acc, label='TEST ACCURACY')\n",
        "plt.legend()"
      ],
      "execution_count": 356,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb50d185b10>"
            ]
          },
          "metadata": {},
          "execution_count": 356
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn1ia39kJRDM"
      },
      "source": [
        "plt.plot(train_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk8QLs8hblwF"
      },
      "source": [
        "# Testing the network\n",
        "\n",
        "Check the result on the test data, this code is re-used from the labs. But adapted to deal with the binary output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHE3ZHN-bNgr"
      },
      "source": [
        "# Evaluate the trained network.\n",
        "\n",
        "def test_accuracy():\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():   # No need for keepnig track of necessary changes to the gradient.\n",
        "    for data in test_dl:\n",
        "      X, y = data\n",
        "      output = net(X.view(-1,13))\n",
        "    \n",
        "      for idx, val in enumerate(output):    \n",
        "      \n",
        "        prediction = val.item()\n",
        "        if prediction < 0.5: prediction = 0\n",
        "        else: prediction = 1\n",
        "\n",
        "      # print('RESULT: ',prediction,y[idx].item())\n",
        "\n",
        "        if prediction == y[idx].item():\n",
        "          correct += 1\n",
        "      \n",
        "        total += 1\n",
        "    # print('Accuracy:', round(correct/total, 3))\n",
        "\n",
        "  return round(correct/total, 3)"
      ],
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxdTXvdKJhVi",
        "outputId": "9d24c33d-1d60-4b46-cb5a-a8c41b415db0"
      },
      "source": [
        "test_accuracy()"
      ],
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.625"
            ]
          },
          "metadata": {},
          "execution_count": 332
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS_dbuYhJd-S"
      },
      "source": [
        "def training_accuracy():\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():   # No need for keepnig track of necessary changes to the gradient.\n",
        "    for data in train_dl:\n",
        "      X, y = data\n",
        "      output = net(X.view(-1,13))\n",
        "    \n",
        "      for idx, val in enumerate(output):    \n",
        "      \n",
        "        prediction = val.item()\n",
        "        if prediction < 0.5: prediction = 0\n",
        "        else: prediction = 1\n",
        "\n",
        "      # print('RESULT: ',prediction,y[idx].item())\n",
        "\n",
        "        if prediction == y[idx].item():\n",
        "          correct += 1\n",
        "      \n",
        "        total += 1\n",
        "    # print('Accuracy:', round(correct/total, 3))\n",
        "\n",
        "  return round(correct/total, 3)"
      ],
      "execution_count": 354,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8mdXVzfbtTL"
      },
      "source": [
        "## Test results\n",
        "\n",
        "Some comments on the different network architectures that we built and tested, what was good and what was bad. Also on the learning parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYI_QucHbOEf"
      },
      "source": [
        "# Exporting the network\n",
        "Here we store the trained network, now it can be deployed in the predictor class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuA2lDDVbbAW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}